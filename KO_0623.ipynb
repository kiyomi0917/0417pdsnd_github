{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiyomi0917/DoSA-PJ/blob/main/KO_0623.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wkE0b0TlkjKI",
        "outputId": "719cc791-8fc5-440c-f843-40ac7d653910"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Janome==0.3.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "kpZsn5fjzKQt",
        "outputId": "a9b150ea-c89b-44e2-a3aa-9cb733aee3d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Janome==0.3.7\n",
            "  Downloading Janome-0.3.7-py27.py3-none-any.whl (20.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.7/20.7 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Janome\n",
            "Successfully installed Janome-0.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from janome.tokenizer import Tokenizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Load the Excel file\n",
        "file_path = '/content/drive/MyDrive/dataset_0619.xlsx'\n",
        "xl = pd.ExcelFile(file_path)"
      ],
      "metadata": {
        "id": "Lj4qz_jIk8g2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the desired sheet into a DataFrame\n",
        "df1 = xl.parse('0619データ')\n",
        "\n",
        "#print(df1) # Print the DataFrame"
      ],
      "metadata": {
        "id": "knasNber_-4A"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# テキストを分割する関数_すべての単語を抽出する\n",
        "t=Tokenizer()\n",
        "def tokenize1(text):\n",
        "    tokens = t.tokenize(text)\n",
        "    noun = []\n",
        "    for token in tokens:\n",
        "        noun.append(token.surface)\n",
        "    return noun\n",
        "\n",
        "# 単語を品詞ごとに抽出\n",
        "def tokenize2(text):\n",
        "    tokens = t.tokenize(text)\n",
        "    noun = []\n",
        "    for token in tokens:\n",
        "        # 「名詞」「動詞」「形容詞」「形容動詞」を取り出す\n",
        "        if token.part_of_speech.split(',')[0] in ['名詞', '動詞', '形容詞', '形容動詞']:\n",
        "            noun.append(token.surface)\n",
        "    return noun\n"
      ],
      "metadata": {
        "id": "99sW_nGX26-D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = df1[\"概要\"]\n",
        "labels = df1[\"ハラスメント判定\"].astype(int)  # ラベルを整数に変換\n",
        "print(labels)"
      ],
      "metadata": {
        "id": "-gkPej1-0Pub",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7779b0e5-5048-49b7-b032-b679b0209e70"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      0\n",
            "1      0\n",
            "2      0\n",
            "3      0\n",
            "4      0\n",
            "      ..\n",
            "108    1\n",
            "109    1\n",
            "110    1\n",
            "111    1\n",
            "112    1\n",
            "Name: ハラスメント判定, Length: 113, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = docs\n",
        "train_labels =labels\n",
        "train_labels = train_labels.astype(int)  # ラベルを整数に変換"
      ],
      "metadata": {
        "id": "WJ8bsxXI2oUG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#from sklearn.metrics import accuracy_score\n",
        "file_path = '/content/drive/MyDrive/dataset_0619.xlsx'\n",
        "xl = pd.ExcelFile(file_path)\n",
        "\n",
        "#Load a sheet into a DataFrame by name\n",
        "df2 = xl.parse('0620データ')\n",
        "\n",
        "# テストデータの概要とラベルを取得\n",
        "test_data = df2[\"概要\"]\n",
        "test_labels = df2[\"ハラスメント判定\"]\n",
        "\n",
        "# テキストデータをベクトル化するためのVectorizerを定義\n",
        "vectorizer = TfidfVectorizer(tokenizer=tokenize2)\n",
        "\n",
        "# ベクトル化に使用するトークン化関数（tokenize1）を指定してVectorizerを初期化\n",
        "vectorizer = TfidfVectorizer(tokenizer=tokenize2)\n",
        "\n",
        "# 学習データをベクトル化\n",
        "train_matrix = vectorizer.fit_transform(train_data)\n",
        "\n",
        "# モデルを設定\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "# パラメータでモデルを再学習\n",
        "model.fit(train_matrix, train_labels)\n",
        "\n",
        "# 訓練データの予測\n",
        "train_predictions = model.predict(train_matrix)\n",
        "\n",
        "# 訓練データの正解率を計算して表示\n",
        "train_accuracy = accuracy_score(train_labels, train_predictions)\n",
        "print(\"訓練データの正解率:\", train_accuracy)\n",
        "\n",
        "# テストデータを変換\n",
        "test_matrix = vectorizer.transform(test_data)\n",
        "\n",
        "# テストデータの予測\n",
        "test_predictions = model.predict(test_matrix)\n",
        "\n",
        "# テストデータの正解率を計算して表示\n",
        "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
        "print(\"テストデータの正解率:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2dFH5T3E57-8",
        "outputId": "479a158f-8583-4257-c77d-9ba87749f19a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "訓練データの正解率: 1.0\n",
            "テストデータの正解率: 0.7727272727272727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ハイパーパラメーターの値の候補を設定\n",
        "model_param_set_grid = {\"n_estimators\": [50, 100, 200],\n",
        "                        \"max_depth\": [None, 10, 20, 30],\n",
        "                        \"random_state\": [42]}\n",
        "\n",
        "# グリッドサーチでハイパーパラメーターを探索\n",
        "clf = GridSearchCV(model, model_param_set_grid)\n",
        "clf.fit(train_matrix, train_labels)\n",
        "\n",
        "# 最適なパラメータとそのときのスコアを表示\n",
        "print(\"ハイパーパラメーター:{}\".format(clf.best_params_))\n",
        "# print(\"ベストスコア:\", clf.best_score_)\n",
        "\n",
        "# 最適なパラメータでモデルを構築\n",
        "best_model = RandomForestClassifier(**clf.best_params_)\n",
        "\n",
        "# 最適なパラメータでモデルを再学習\n",
        "best_model.fit(train_matrix, train_labels)\n",
        "\n",
        "# 訓練データの予測\n",
        "train_predictions = best_model.predict(train_matrix)\n",
        "\n",
        "# 訓練データの正解率を計算して表示\n",
        "train_accuracy = accuracy_score(train_labels, train_predictions)\n",
        "print(\"訓練データの正解率:\", train_accuracy)\n",
        "\n",
        "# テストデータを変換/予測\n",
        "test_matrix = vectorizer.transform(test_data)\n",
        "test_predictions = best_model.predict(test_matrix)\n",
        "\n",
        "# テストデータの正解率を計算して表示\n",
        "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
        "print(\"テストデータの正解率:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CHQboWDnWo_K",
        "outputId": "baab4002-931b-45aa-e78d-c70ff90b2dde"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ハイパーパラメーター:{'max_depth': None, 'n_estimators': 50, 'random_state': 42}\n",
            "訓練データの正解率: 1.0\n",
            "テストデータの正解率: 0.7727272727272727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn import svm\n",
        "import pickle\n",
        "\n",
        "#学習モデルの保存\n",
        "with open('model.pickle', mode='wb') as f:\n",
        "    pickle.dump(best_model,f,protocol=2)\n"
      ],
      "metadata": {
        "id": "wHCC10Sgv40L"
      },
      "execution_count": 10,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}